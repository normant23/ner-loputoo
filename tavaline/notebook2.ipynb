{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "6c78baf3-b1db-4319-bf68-bf6869af1b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install simpletransformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4e4dcf34-57bb-40e3-9397-d5224766f4be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:config.py:54: PyTorch version 2.5.1+cu124 available.\n"
     ]
    }
   ],
   "source": [
    "import estnltk\n",
    "import os\n",
    "import time\n",
    "\n",
    "from estnltk import Text\n",
    "import re\n",
    "from estnltk.converters import text_to_json, json_to_text\n",
    "\n",
    "from estnltk_core.layer.span_operations import conflict\n",
    "import pandas as pd\n",
    "\n",
    "# Import adapted words tokenization\n",
    "from words_tokenization import preprocess_words # vaja words_tokenization.py\n",
    "\n",
    "# Import adapted sentence tokenizer\n",
    "from sentence_tokenization import sentence_tokenizer # vaja sentence_tokenization.py\n",
    "from sentence_tokenization import postfix_sentence_breaks_inside_parentheses # vaja sentence_tokenization.py\n",
    "\n",
    "from estnltk_core.layer_operations import extract_sections\n",
    "\n",
    "import logging\n",
    "from simpletransformers.ner import NERModel, NERArgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "49df779a-e9fd-4bb4-89d1-4035b40abfdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "OutputFolder = 'data/output4'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7d4b2346-0ce5-4921-9349-349be084a495",
   "metadata": {},
   "outputs": [],
   "source": [
    "#output2 -> output3, vana words,sentences layer maha ja uus peale\n",
    "for name in os.listdir('data/output2'):\n",
    "    with open(os.path.join('data/output2', name), \"r\", encoding=\"utf-8\") as file:\n",
    "        content = file.read()\n",
    "    text_import = json_to_text(json_text=content)\n",
    "\n",
    "    #print(name, len(text_import.words), len(text_import.sentences))\n",
    "    #print(text_import.layers)\n",
    "    text_import.pop_layer('sentences')\n",
    "    text_import.pop_layer('words')\n",
    "    #print(text_import.layers)\n",
    "\n",
    "    preprocess_words( text_import )\n",
    "    sentence_tokenizer.tag( text_import )\n",
    "    postfix_sentence_breaks_inside_parentheses( text_import, doc_name='' )\n",
    "    #print(len(text_import.words), len(text_import.sentences))\n",
    "\n",
    "    text_to_json(text_import, file=('data/output3' + \"/\" + name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6ebc37fd-d06d-479a-9f8a-b4b119066981",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Span('htusegümnaasiumi', [{'tag': 'ORG'}]) 39589\n",
      "Span('õhtusegümnaasiumi', [{'tag': 'ORG'}]) 39588\n",
      "Span('Rosenfeldt', [{'tag': 'PER'}]) 39097\n",
      "Span('J.Rosenfeldt', [{'tag': 'PER'}]) 39095\n",
      "Span('majõevaheline plats', [{'tag': 'LOC'}]) 54118\n",
      "Span('Emajõevaheline plats', [{'tag': 'LOC'}]) 54117\n"
     ]
    }
   ],
   "source": [
    "#output3 -> output4, fix some mistakes\n",
    "for name in os.listdir('data/output3'):\n",
    "    with open(os.path.join('data/output3', name), \"r\", encoding=\"utf-8\") as file:\n",
    "        content = file.read()\n",
    "    text_import = json_to_text(json_text=content)\n",
    "\n",
    "    if name == '1934-10-15_manual_annotated.json':\n",
    "        #htusegümnaasiumi -> õhtusegümnaasiumi\n",
    "        print(text_import.ne_gold_a[642], text_import.ne_gold_a[642].start)\n",
    "        text_import.ne_gold_a.remove_span(text_import.ne_gold_a[642])\n",
    "        text_import.ne_gold_a.add_annotation( text_import.words[7005].base_span, {'tag': 'ORG'})\n",
    "        print(text_import.ne_gold_a[642], text_import.ne_gold_a[642].start)\n",
    "    \n",
    "    if name == '1935-09-30_manual_annotated.json':\n",
    "        #Rosenfeldt -> J.Rosenfeldt\n",
    "        print(text_import.ne_gold_a[431], text_import.ne_gold_a[431].start)\n",
    "        text_import.ne_gold_a.remove_span(text_import.ne_gold_a[431])\n",
    "        text_import.ne_gold_a.add_annotation( text_import.words[6679].base_span, {'tag': 'PER'})\n",
    "        print(text_import.ne_gold_a[431], text_import.ne_gold_a[431].start)\n",
    "\n",
    "        #majõevaheline plats -> Emajõevaheline plats\n",
    "        print(text_import.ne_gold_a[597], text_import.ne_gold_a[597].start)\n",
    "        text_import.ne_gold_a.remove_span(text_import.ne_gold_a[597])\n",
    "        text_import.ne_gold_a.add_annotation( (54117, 54137), {'tag': 'LOC'})\n",
    "        print(text_import.ne_gold_a[597], text_import.ne_gold_a[597].start)\n",
    "    \n",
    "    text_to_json(text_import, file=(OutputFolder + \"/\" + name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "42fe6009-212a-4db1-9974-a2bcdebba000",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>words</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1922-04-24_manual_annotated.json</td>\n",
       "      <td>0</td>\n",
       "      <td>Tartu</td>\n",
       "      <td>B-ORG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1922-04-24_manual_annotated.json</td>\n",
       "      <td>0</td>\n",
       "      <td>linnawolikogu</td>\n",
       "      <td>I-ORG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1922-04-24_manual_annotated.json</td>\n",
       "      <td>0</td>\n",
       "      <td>korraline</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1922-04-24_manual_annotated.json</td>\n",
       "      <td>0</td>\n",
       "      <td>koosolek</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1922-04-24_manual_annotated.json</td>\n",
       "      <td>0</td>\n",
       "      <td>esmaspäewal</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51241</th>\n",
       "      <td>1941-01-03_manual_annotated.json</td>\n",
       "      <td>42</td>\n",
       "      <td>.</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51242</th>\n",
       "      <td>1941-01-03_manual_annotated.json</td>\n",
       "      <td>43</td>\n",
       "      <td>Ärakiri</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51243</th>\n",
       "      <td>1941-01-03_manual_annotated.json</td>\n",
       "      <td>43</td>\n",
       "      <td>õige</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51244</th>\n",
       "      <td>1941-01-03_manual_annotated.json</td>\n",
       "      <td>43</td>\n",
       "      <td>:</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51245</th>\n",
       "      <td>1941-01-03_manual_annotated.json</td>\n",
       "      <td>43</td>\n",
       "      <td>Sekretär</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>51246 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                               filename  sentence_id          words labels\n",
       "0      1922-04-24_manual_annotated.json            0          Tartu  B-ORG\n",
       "1      1922-04-24_manual_annotated.json            0  linnawolikogu  I-ORG\n",
       "2      1922-04-24_manual_annotated.json            0      korraline      O\n",
       "3      1922-04-24_manual_annotated.json            0       koosolek      O\n",
       "4      1922-04-24_manual_annotated.json            0    esmaspäewal      O\n",
       "...                                 ...          ...            ...    ...\n",
       "51241  1941-01-03_manual_annotated.json           42              .      O\n",
       "51242  1941-01-03_manual_annotated.json           43        Ärakiri      O\n",
       "51243  1941-01-03_manual_annotated.json           43           õige      O\n",
       "51244  1941-01-03_manual_annotated.json           43              :      O\n",
       "51245  1941-01-03_manual_annotated.json           43       Sekretär      O\n",
       "\n",
       "[51246 rows x 4 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def next_tag(text_import, tagnr):\n",
    "    tags = {\n",
    "        \"PER\": \"PER\",\n",
    "        \"LOC\": \"LOC\",\n",
    "        \"ORG\": \"ORG\",\n",
    "        \"ORG_POL\": \"ORG\",\n",
    "        \"ORG_GPE\": \"ORG\",\n",
    "    }\n",
    "    \n",
    "    tagnr += 1\n",
    "    \n",
    "    while True:\n",
    "        if tagnr >= len(text_import.ne_gold_a):\n",
    "            return -1\n",
    "        if text_import.ne_gold_a[tagnr].tag in tags:\n",
    "            return tagnr\n",
    "        else:\n",
    "            tagnr += 1\n",
    "\n",
    "def process_file(file_name):\n",
    "    with open(os.path.join(OutputFolder, file_name), \"r\", encoding=\"utf-8\") as file:\n",
    "        content = file.read()\n",
    "    \n",
    "    text_import = json_to_text(json_text=content)\n",
    "    \n",
    "    data = []\n",
    "    tag_nr = next_tag(text_import, -1)\n",
    "    sen_nr = 0\n",
    "    \n",
    "    tags = {\n",
    "        \"PER\": \"PER\",\n",
    "        \"LOC\": \"LOC\",\n",
    "        \"ORG\": \"ORG\",\n",
    "        \"ORG_POL\": \"ORG\",\n",
    "        \"ORG_GPE\": \"ORG\",\n",
    "    }\n",
    "\n",
    "    # Process each word\n",
    "    for word in text_import.words:\n",
    "        # Update sentence ID if needed\n",
    "        if sen_nr < len(text_import.sentences) and not conflict(text_import.sentences[sen_nr], word):\n",
    "            sen_nr += 1\n",
    "\n",
    "        if tag_nr != -1:\n",
    "            tag = text_import.ne_gold_a[tag_nr]\n",
    "            \n",
    "            # If word is before the current entity\n",
    "            if word.end <= tag.start:\n",
    "                # Add as regular word\n",
    "                data.append([file_name, sen_nr, word.text, \"O\"])\n",
    "\n",
    "            if word.start == tag.start:\n",
    "                normalized_tag = tags.get(tag.tag, tag.tag)\n",
    "                data.append([file_name, sen_nr, word.text, f\"B-{normalized_tag}\"])\n",
    "            # If word is part of the current entity\n",
    "            elif conflict(tag, word):\n",
    "                normalized_tag = tags.get(tag.tag, tag.tag)\n",
    "                data.append([file_name, sen_nr, word.text, f\"I-{normalized_tag}\"])\n",
    "                \n",
    "            # If word is after the current entity\n",
    "            elif word.start >= tag.end:\n",
    "                tag_nr = next_tag(text_import, tag_nr)\n",
    "                \n",
    "                if tag_nr != -1:\n",
    "                    tag = text_import.ne_gold_a[tag_nr]\n",
    "\n",
    "                    if word.start == tag.start:\n",
    "                        data.append([file_name, sen_nr, word.text, f\"B-{normalized_tag}\"])\n",
    "                    else:\n",
    "                        data.append([file_name, sen_nr, word.text, \"O\"])\n",
    "                else:\n",
    "                    data.append([file_name, sen_nr, word.text, \"O\"])\n",
    "        else:\n",
    "            data.append([file_name, sen_nr, word.text, \"O\"])\n",
    "    \n",
    "    return data\n",
    "\n",
    "def process_directory_to_dict(output_folder):\n",
    "    all_data = []\n",
    "    \n",
    "    for name in os.listdir(output_folder):\n",
    "        all_data.extend(process_file(name))\n",
    "\n",
    "    data = pd.DataFrame(all_data, columns=[\"filename\", \"sentence_id\", \"words\", \"labels\"])\n",
    "    return data\n",
    "\n",
    "data = process_directory_to_dict('data/output4')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f9648d1a-35b1-420e-968f-f8474bbbbe0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True 2219 2219\n",
      "True 7236 7236\n",
      "True 12795 12795\n",
      "True 8981 8981\n",
      "True 11739 11739\n",
      "True 7585 7585\n",
      "True 691 691\n"
     ]
    }
   ],
   "source": [
    "#Testing if every word is present in the dataframe\n",
    "for name in os.listdir(OutputFolder):\n",
    "    with open(os.path.join(OutputFolder, name), \"r\", encoding=\"utf-8\") as file:\n",
    "        content = file.read()\n",
    "    text_import = json_to_text(json_text=content)\n",
    "\n",
    "    print(len(data[data[\"filename\"] == name]) == len(text_import.words), len(data[data[\"filename\"] == name]), len(text_import.words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ee941d44-7f2b-405d-9fe8-397cc67a034c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True 166 166 1922-04-24_manual_annotated.json\n",
      "----------\n",
      "True 495 495 1927-03-28_manual_annotated.json\n",
      "----------\n",
      "True 723 723 1932-01-25_manual_annotated.json\n",
      "----------\n",
      "True 527 527 1934-10-15_manual_annotated.json\n",
      "----------\n",
      "True 562 562 1935-09-30_manual_annotated.json\n",
      "----------\n",
      "True 323 323 1936-09-07_manual_annotated.json\n",
      "----------\n",
      "True 37 37 1941-01-03_manual_annotated.json\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "#Testing if correct numbers of tags are in the dataset and if tehre are any mismatches\n",
    "for name in os.listdir(OutputFolder):\n",
    "    with open(os.path.join(OutputFolder, name), \"r\", encoding=\"utf-8\") as file:\n",
    "        content = file.read()\n",
    "    text_import = json_to_text(json_text=content)\n",
    "\n",
    "    tags = {\n",
    "        \"PER\": \"PER\",\n",
    "        \"LOC\": \"LOC\",\n",
    "        \"ORG\": \"ORG\",\n",
    "        \"ORG_POL\": \"ORG\",\n",
    "        \"ORG_GPE\": \"ORG\",\n",
    "    }\n",
    "    \n",
    "    starting_data = [[i.text, tags[i.tag], i.start] for i in text_import.ne_gold_a if i.tag in list(tags.keys())]\n",
    "    print(\n",
    "        len(data[(data[\"filename\"] == name) & (data[\"labels\"].str.startswith('B-'))]) == len(starting_data),\n",
    "        len(data[(data[\"filename\"] == name) & (data[\"labels\"].str.startswith('B-'))]),\n",
    "        len(starting_data),\n",
    "        name\n",
    "    )\n",
    "    print('----------')\n",
    "    for nr, (idx, row) in enumerate(data[(data[\"filename\"] == name) & (data[\"labels\"].str.startswith('B-'))].iterrows()):\n",
    "        if starting_data[nr][0].startswith(row['words']) == False:\n",
    "            print(nr, starting_data[nr], row, idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "136d4309-4ced-4dbd-b9cb-88df6d6ad977",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "name = '1935-09-30_manual_annotated.json'\n",
    "with open(os.path.join('data/output4', name), \"r\", encoding=\"utf-8\") as file:\n",
    "        content = file.read()\n",
    "text_import = json_to_text(json_text=content)\n",
    "text_import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "b13c92d2-43ff-4358-8814-4e32062d7bcc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "230 Kui O\n",
      "230 seda O\n",
      "230 maja O\n",
      "230 linna O\n",
      "230 asutiste O\n",
      "230 jaoks O\n",
      "230 kõigiti O\n",
      "230 otstarbekohaseks O\n",
      "230 muuta O\n",
      "230 osutub O\n",
      "230 paratamatuks O\n",
      "230 ette O\n",
      "230 võtta O\n",
      "230 sisemisi O\n",
      "230 ümberehitusi O\n",
      "230 , O\n",
      "230 samuti O\n",
      "230 juure O\n",
      "230 ehitada O\n",
      "230 uusi O\n",
      "230 ruume O\n",
      "230 , O\n",
      "230 milleks O\n",
      "230 on O\n",
      "230 olemas O\n",
      "230 soodsad O\n",
      "230 võimalused O\n",
      "230 . O\n",
      "231 Ümber- O\n",
      "231 ja O\n",
      "231 juureehituste O\n",
      "231 kohta O\n",
      "231 , O\n",
      "231 kui O\n",
      "231 see O\n",
      "231 maja O\n",
      "231 omandatakse O\n",
      "231 linnale O\n",
      "231 , O\n",
      "231 koostatakse O\n",
      "231 vastavad O\n",
      "231 kavad O\n",
      "231 ja O\n",
      "231 kulude O\n",
      "231 eelarved O\n",
      "231 ja O\n",
      "231 need O\n",
      "231 esitatakseLinnavolikogule I-ORG\n",
      "231 kinnitamiseks O\n",
      "231 . O\n",
      "232 Nimetatud O\n",
      "232 maja O\n",
      "232 linnale O\n",
      "232 omandamise O\n",
      "232 korral O\n",
      "232 on O\n",
      "232 võimalik O\n",
      "232 katta O\n",
      "232 ostuhinda O\n",
      "232 ja O\n",
      "232 ostuga O\n",
      "232 seotud O\n",
      "232 kulusid O\n",
      "232 järgmiselt O\n",
      "232 : O\n",
      "232 1. O\n",
      "233 Ostuhinnast O\n",
      "233 lepingu O\n",
      "233 sõlmimisel O\n",
      "233 tasumisele O\n",
      "233 kuuluvad O\n",
      "233 kr O\n",
      "233 . O\n",
      "233 30.000 O\n",
      "233 ja O\n",
      "233 kinnistamisega O\n",
      "233 ( O\n",
      "233 kinnist O\n",
      "233 . O\n",
      "233 lõiv O\n",
      "233 6% O\n",
      "233 , O\n",
      "233 tempelmaks O\n",
      "233 1% O\n",
      "233 ja O\n",
      "233 muud O\n",
      "233 väiksemad O\n",
      "233 kulud O\n",
      "233 ) O\n",
      "233 ning O\n",
      "233 obligatsioonide O\n",
      "233 tegemisega O\n",
      "233 seotud O\n",
      "233 kulud O\n",
      "233 kuni O\n",
      "233 kr O\n",
      "233 . O\n",
      "233 10.000 O\n",
      "233 , O\n",
      "233 - O\n",
      "233 kokku O\n",
      "233 kr O\n",
      "233 . O\n",
      "233 40.000 O\n",
      "233 kaetakse O\n",
      "233 Linnavolikogu B-ORG\n",
      "233 otsustel O\n",
      "233 25. O\n",
      "233 apr. O\n",
      "233 ja O\n",
      "233 24. O\n",
      "233 oktoobr O\n",
      "233 . O\n",
      "233 1932 O\n",
      "233 Pikalaenu B-ORG\n",
      "233 Pangast I-ORG\n",
      "233 kr O\n",
      "233 . O\n",
      "233 140.000 O\n",
      "233 suuruses O\n",
      "233 saadud O\n",
      "233 laenust O\n",
      "233 , O\n",
      "233 mille O\n",
      "233 kindlustuseks O\n",
      "233 laenulepingute O\n",
      "233 järgi O\n",
      "233 7. O\n",
      "233 nov. O\n",
      "233 1932 O\n",
      "233 on O\n",
      "233 antud O\n",
      "233 pangale O\n",
      "233 obligatsioonid O\n",
      "233 ja O\n",
      "233 mis O\n",
      "233 tasutakse O\n",
      "233 igaaastaste O\n",
      "233 osadega O\n",
      "233 30. O\n",
      "233 juuniks O\n",
      "233 1944 O\n",
      "233 , O\n",
      "233 ja O\n",
      "233 2. O\n",
      "234 Ülejäänud O\n",
      "234 ostuhinna O\n",
      "234 osa O\n",
      "234 - O\n",
      "234 kr O\n",
      "234 . O\n",
      "234 50.000 O\n",
      "234 kaetakse O\n",
      "234 eelarve O\n",
      "234 korras O\n",
      "234 viie O\n",
      "234 aasta O\n",
      "234 jooksul O\n",
      "234 iga O\n",
      "234 aasta O\n",
      "234 kr O\n",
      "234 . O\n",
      "234 10.000 O\n",
      "234 , O\n",
      "234 - O\n",
      "234 ühes O\n",
      "234 5% O\n",
      "234 % O\n",
      "234 aastas O\n",
      "234 tasumata O\n",
      "234 osa O\n",
      "234 pealt O\n",
      "234 . O\n"
     ]
    }
   ],
   "source": [
    "for (idx, row) in data[(data[\"filename\"] == '1934-10-15_manual_annotated.json') & (data[\"sentence_id\"] > 229) & (data[\"sentence_id\"] < 235)].iterrows():\n",
    "    print(row['sentence_id'], row['words'], row['labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "681c989f-27d1-4356-a305-18b656bbc1a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "370 Span('Tammeväljal', [{'tag': 'LOC'}])\n",
      "500 Span('Tamme linnaosas', [{'tag': 'LOC'}])\n",
      "510 Span('Tamme linnaosas', [{'tag': 'LOC'}])\n",
      "735 Span('Tamme', [{'tag': 'LOC'}])\n",
      "747 Span('Tamme linnaosa', [{'tag': 'LOC'}])\n",
      "775 Span('Tamme l.o.', [{'tag': 'LOC'}])\n",
      "794 Span('Tammelinnaosa', [{'tag': 'LOC'}])\n",
      "822 Span('Tamme l.o.', [{'tag': 'LOC'}])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Tartu'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name = '1935-09-30_manual_annotated.json'\n",
    "with open(os.path.join('output4', name), \"r\", encoding=\"utf-8\") as file:\n",
    "        content = file.read()\n",
    "text_import = json_to_text(json_text=content)\n",
    "for nr,i in enumerate(text_import.ne_gold_a):\n",
    "    if i.text.startswith('Tamme'):\n",
    "    #if i.text == \"esitatakseLinnavolikogule\":\n",
    "    #if i.start == 39588:\n",
    "        print(nr, i)\n",
    "#text_import.words[6679].base_span \n",
    "text_import.tokens[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ebf4b235-7703-48f4-b6d2-a60fc1b9e849",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64813 64827 Tamme linnaosa\n",
      "66479 66707 EnvelopingSpan(['Linnavolikogu', ',', 'olles', 'asja', 'ettekande', 'ära', 'kuulanud', ',', 'lahtisel', 'hääletusel', 'ühel', 'häälel', 'otsustab', ':', '1', ')', 'Kinnitada', 'allpool', 'tähendatud', 'ehituskruntide', 'saajateks', 'järgmised', 'isikud', ':', 'a', ')', 'Tamme', 'linnaosa', 'maa-alal', ',', 'kinn', '.', 'nr.', '1', '(', 'IV', 'hüp', '.', 'ringkond', ')', ':', 'kvart', '.'], [{}])\n",
      "Span('Tamme', [{'normalized_form': None}])\n"
     ]
    }
   ],
   "source": [
    "name = '1935-09-30_manual_annotated.json'\n",
    "with open(os.path.join('output4', name), \"r\", encoding=\"utf-8\") as file:\n",
    "        content = file.read()\n",
    "text_import = json_to_text(json_text=content)\n",
    "#print(text_import.sentences[231].start, text_import.sentences[231].end)\n",
    "#print(text_import.ne_gold_a[364])\n",
    "#sections = extract_sections(text_import, sections=[(54118,54128)])\n",
    "#sections\n",
    "#for nr, i in enumerate(text_import.ne_gold_a):\n",
    "#    if i.text == \"htusegümnaasiumi\":\n",
    "#        print(nr, i, i.start, i.end)\n",
    "#print(text_import.ne_gold_a[642].start, text_import.ne_gold_a[642].text)\n",
    "#print(text_import.ne_gold_a[747].start, text_import.ne_gold_a[747].end, text_import.ne_gold_a[747].text)\n",
    "#print(text_import.sentences[668].start, text_import.sentences[668].end, text_import.sentences[668])\n",
    "#print(text_import.words[11207])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3e13dd14-926e-4283-8b65-2373f58ce2bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "133\n",
      "461\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>words</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>43004</th>\n",
       "      <td>133</td>\n",
       "      <td>Juhatab</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43005</th>\n",
       "      <td>133</td>\n",
       "      <td>Volikogu</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43006</th>\n",
       "      <td>133</td>\n",
       "      <td>juhataja</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43007</th>\n",
       "      <td>133</td>\n",
       "      <td>A.Piip</td>\n",
       "      <td>B-PER</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43008</th>\n",
       "      <td>133</td>\n",
       "      <td>.</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43009</th>\n",
       "      <td>134</td>\n",
       "      <td>Koos</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43010</th>\n",
       "      <td>134</td>\n",
       "      <td>on</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43011</th>\n",
       "      <td>134</td>\n",
       "      <td>33</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43012</th>\n",
       "      <td>134</td>\n",
       "      <td>linnavolinikust</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43013</th>\n",
       "      <td>134</td>\n",
       "      <td>25</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43014</th>\n",
       "      <td>134</td>\n",
       "      <td>linnavolinikku</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43015</th>\n",
       "      <td>134</td>\n",
       "      <td>ja</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43016</th>\n",
       "      <td>134</td>\n",
       "      <td>väljaspoolt</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43017</th>\n",
       "      <td>134</td>\n",
       "      <td>Volikogu</td>\n",
       "      <td>B-ORG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43018</th>\n",
       "      <td>134</td>\n",
       "      <td>koosseisu</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43019</th>\n",
       "      <td>134</td>\n",
       "      <td>määratud</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43020</th>\n",
       "      <td>134</td>\n",
       "      <td>linnapea</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43021</th>\n",
       "      <td>134</td>\n",
       "      <td>A.Tõnisson</td>\n",
       "      <td>B-PER</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43022</th>\n",
       "      <td>134</td>\n",
       "      <td>ja</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43023</th>\n",
       "      <td>134</td>\n",
       "      <td>linnanõunik</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43024</th>\n",
       "      <td>134</td>\n",
       "      <td>J.Roo</td>\n",
       "      <td>B-PER</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43025</th>\n",
       "      <td>134</td>\n",
       "      <td>.</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       sentence_id            words labels\n",
       "43004          133          Juhatab      O\n",
       "43005          133         Volikogu      O\n",
       "43006          133         juhataja      O\n",
       "43007          133           A.Piip  B-PER\n",
       "43008          133                .      O\n",
       "43009          134             Koos      O\n",
       "43010          134               on      O\n",
       "43011          134               33      O\n",
       "43012          134  linnavolinikust      O\n",
       "43013          134               25      O\n",
       "43014          134   linnavolinikku      O\n",
       "43015          134               ja      O\n",
       "43016          134      väljaspoolt      O\n",
       "43017          134         Volikogu  B-ORG\n",
       "43018          134        koosseisu      O\n",
       "43019          134         määratud      O\n",
       "43020          134         linnapea      O\n",
       "43021          134       A.Tõnisson  B-PER\n",
       "43022          134               ja      O\n",
       "43023          134      linnanõunik      O\n",
       "43024          134            J.Roo  B-PER\n",
       "43025          134                .      O"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name = '1922-04-24_manual_annotated.json'\n",
    "with open(os.path.join('output4', name), \"r\", encoding=\"utf-8\") as file:\n",
    "        content = file.read()\n",
    "text_import = json_to_text(json_text=content)\n",
    "print(len(text_import.sentences))\n",
    "name = '1936-09-07_manual_annotated.json'\n",
    "with open(os.path.join('output4', name), \"r\", encoding=\"utf-8\") as file:\n",
    "        content = file.read()\n",
    "text_import = json_to_text(json_text=content)\n",
    "print(len(text_import.sentences))\n",
    "\n",
    "data = process_directory_to_dict('output4')\n",
    "new = getData(data, ['1922-04-24_manual_annotated.json', '1936-09-07_manual_annotated.json'])\n",
    "new[(new['sentence_id'] > 132) & (new['sentence_id'] < 135)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "69988ccd-603b-44fe-b6e6-39e01bbecad0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['asi', 'Tammelinnaosamaa', '-', 'alal', 'asi2']\n",
      "['asi', 'Tamme', 'linnaosamaa', '-', 'alal', 'asi2']\n",
      "['asi', 'Tamme', 'linnaosa', 'maa', '-', 'alal', 'asi2']\n",
      "['asi', 'Tamme', 'linnaosa', 'maa-alal', 'asi2']\n"
     ]
    }
   ],
   "source": [
    "from estnltk.taggers import TokenSplitter\n",
    "import re\n",
    "\n",
    "# Create a pattern that captures the components you want to split\n",
    "pattern1 = re.compile(r'(?P<end>Tamme)linnaosamaa')\n",
    "\n",
    "# Initialize the TokenSplitter with your pattern\n",
    "token_splitter1 = TokenSplitter(patterns=[pattern1])\n",
    "\n",
    "pattern2 = re.compile(r'(?P<end>linnaosa)maa')\n",
    "token_splitter2 = TokenSplitter(patterns=[pattern2])\n",
    "\n",
    "# Apply the splitter to your text\n",
    "text = \"asi Tammelinnaosamaa-alal asi2\"\n",
    "# You'd typically create a Text object from estnltk first\n",
    "#from estnltk import Text\n",
    "text_obj = Text(text)\n",
    "text_obj.tag_layer('tokens')\n",
    "print(text_obj.tokens.text)\n",
    "token_splitter1.retag(text_obj)\n",
    "print(text_obj.tokens.text)\n",
    "token_splitter2.retag(text_obj)\n",
    "print(text_obj.tokens.text)\n",
    "preprocess_words( text_obj )\n",
    "sentence_tokenizer.tag( text_obj )\n",
    "postfix_sentence_breaks_inside_parentheses( text_obj, doc_name='' )\n",
    "print(text_obj.words.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b347016f-13f0-4951-ad70-cc1b6673ce73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1922-04-24_manual_annotated.json - Total: 166; LOC: 32 PER: 56 ORG: 78\n",
      "1927-03-28_manual_annotated.json - Total: 495; LOC: 16 PER: 189 ORG: 290\n",
      "1932-01-25_manual_annotated.json - Total: 723; LOC: 70 PER: 399 ORG: 254\n",
      "1934-10-15_manual_annotated.json - Total: 527; LOC: 56 PER: 185 ORG: 286\n",
      "1935-09-30_manual_annotated.json - Total: 562; LOC: 121 PER: 173 ORG: 268\n",
      "1936-09-07_manual_annotated.json - Total: 323; LOC: 25 PER: 76 ORG: 222\n",
      "1941-01-03_manual_annotated.json - Total: 37; LOC: 2 PER: 10 ORG: 25\n"
     ]
    }
   ],
   "source": [
    "def stats():\n",
    "    data = process_directory_to_dict('output4')\n",
    "    for file in data['filename'].unique():\n",
    "        print(\n",
    "            file, \"- Total:\", str(len(data[(data[\"filename\"] == file) & (data[\"labels\"].str.startswith('B-'))])) + \";\",\n",
    "            \"LOC:\", str(len(data[(data[\"filename\"] == file) & (data[\"labels\"] == \"B-LOC\")])),\n",
    "            \"PER:\", str(len(data[(data[\"filename\"] == file) & (data[\"labels\"] == \"B-PER\")])),\n",
    "            \"ORG:\", str(len(data[(data[\"filename\"] == file) & (data[\"labels\"] == \"B-ORG\")])),\n",
    "        )\n",
    "stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "23239ee2-8301-4d23-9648-54f238072126",
   "metadata": {},
   "outputs": [],
   "source": [
    "groups = [['1922-04-24_manual_annotated.json', '1936-09-07_manual_annotated.json'], \n",
    "          ['1927-03-28_manual_annotated.json', '1941-01-03_manual_annotated.json'], \n",
    "          ['1932-01-25_manual_annotated.json'], \n",
    "          ['1934-10-15_manual_annotated.json'], \n",
    "          ['1935-09-30_manual_annotated.json']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9882c6c6-845c-42d3-985b-c0cb5aa73b05",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at tartuNLP/EstBERT and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:ner_model.py:1884:  Converting to features started.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9507f348ce5a48f390c972733a25c335",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef8f9aac28bb45e0a4bef46d910c48a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Norman\\anaconda3\\Lib\\site-packages\\simpletransformers\\ner\\ner_model.py:758: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = amp.GradScaler()\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e0fccd316ea4914a7183ed40375f3ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Epoch 1 of 1:   0%|          | 0/168 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Norman\\anaconda3\\Lib\\site-packages\\simpletransformers\\ner\\ner_model.py:782: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with amp.autocast():\n",
      "C:\\Users\\Norman\\anaconda3\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:224: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:ner_model.py:1884:  Converting to features started.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84ba7041e35046ab95658ac968d197ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90d6cda3662e40bcba2ea3b7d48a88aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Evaluation:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Norman\\anaconda3\\Lib\\site-packages\\simpletransformers\\ner\\ner_model.py:1303: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with amp.autocast():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:ner_model.py:523:  Training of bert model complete. Saved to outputs/.\n",
      "INFO:ner_model.py:1884:  Converting to features started.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e3672af326440b3b7ca846928be80fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef7be8f70a084e55accf846fdd4bac88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Evaluation:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:ner_model.py:1253: {'eval_loss': 0.14548455892751613, 'precision': 0.7654584221748401, 'recall': 0.7223340040241448, 'f1_score': 0.7432712215320911}\n",
      "{'eval_loss': 0.14548455892751613, 'precision': 0.7654584221748401, 'recall': 0.7223340040241448, 'f1_score': 0.7432712215320911}\n",
      "INFO:ner_model.py:1884:  Converting to features started.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db807f03c39d4a6c9023e584678d2a20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b005be5cca64386975386c37c99a782",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Norman\\anaconda3\\Lib\\site-packages\\simpletransformers\\ner\\ner_model.py:758: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = amp.GradScaler()\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "676e56638813467598f07442b86d7737",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Epoch 1 of 1:   0%|          | 0/168 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Norman\\anaconda3\\Lib\\site-packages\\simpletransformers\\ner\\ner_model.py:782: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with amp.autocast():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:ner_model.py:1884:  Converting to features started.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "696bed84a28943f9aa396aa5bb220411",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12046f29c6514d0ba19169d6f4fea4ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Evaluation:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Norman\\anaconda3\\Lib\\site-packages\\simpletransformers\\ner\\ner_model.py:1303: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with amp.autocast():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:ner_model.py:523:  Training of bert model complete. Saved to outputs/.\n",
      "INFO:ner_model.py:1884:  Converting to features started.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e7dfa5689b04b72aae49fb2d288a97d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9b517db50764c04ba57ef668cd6547a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Evaluation:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:ner_model.py:1253: {'eval_loss': 0.08943464447345052, 'precision': 0.7786885245901639, 'recall': 0.8747697974217311, 'f1_score': 0.823937554206418}\n",
      "{'eval_loss': 0.08943464447345052, 'precision': 0.7786885245901639, 'recall': 0.8747697974217311, 'f1_score': 0.823937554206418}\n",
      "INFO:ner_model.py:1884:  Converting to features started.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c355c79a039447fbbf96ba4eefbcb612",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1405596637db47e0bff292b61275ffcf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Norman\\anaconda3\\Lib\\site-packages\\simpletransformers\\ner\\ner_model.py:758: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = amp.GradScaler()\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81782f2d9bfb4410b38a339ba85a8815",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Epoch 1 of 1:   0%|          | 0/153 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Norman\\anaconda3\\Lib\\site-packages\\simpletransformers\\ner\\ner_model.py:782: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with amp.autocast():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:ner_model.py:1884:  Converting to features started.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "105e69830d5a4f3c93316935681017fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39c32b2c5ecd4f6f9ecbc14a6e1889c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Evaluation:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Norman\\anaconda3\\Lib\\site-packages\\simpletransformers\\ner\\ner_model.py:1303: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with amp.autocast():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:ner_model.py:523:  Training of bert model complete. Saved to outputs/.\n",
      "INFO:ner_model.py:1884:  Converting to features started.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b280fbc118904c608ec1073f5d23b8f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "946dea8014b6443191bb287f52253fdc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Evaluation:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:ner_model.py:1253: {'eval_loss': 0.03965941019770172, 'precision': 0.9127324749642346, 'recall': 0.8910614525139665, 'f1_score': 0.9017667844522967}\n",
      "{'eval_loss': 0.03965941019770172, 'precision': 0.9127324749642346, 'recall': 0.8910614525139665, 'f1_score': 0.9017667844522967}\n",
      "INFO:ner_model.py:1884:  Converting to features started.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e1d1a1ee4ea4276923c8cc5ea9a6933",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec3236ecb8b04bb7a1fea22efd412dcd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Norman\\anaconda3\\Lib\\site-packages\\simpletransformers\\ner\\ner_model.py:758: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = amp.GradScaler()\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5adc87232a69494e89f68018f43d85f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Epoch 1 of 1:   0%|          | 0/173 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Norman\\anaconda3\\Lib\\site-packages\\simpletransformers\\ner\\ner_model.py:782: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with amp.autocast():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:ner_model.py:1884:  Converting to features started.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2fa34049ecf74d2896a4b7f558f43aa7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b42ab2e8063a4af68836a47b7481ee01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Evaluation:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Norman\\anaconda3\\Lib\\site-packages\\simpletransformers\\ner\\ner_model.py:1303: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with amp.autocast():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:ner_model.py:523:  Training of bert model complete. Saved to outputs/.\n",
      "INFO:ner_model.py:1884:  Converting to features started.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8cc0c75f92844a3b9aac882d029db69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6f7675b144049d6a8b32e5c3efa03bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Evaluation:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:ner_model.py:1253: {'eval_loss': 0.02202334914666911, 'precision': 0.9438202247191011, 'recall': 0.9563567362428842, 'f1_score': 0.9500471253534402}\n",
      "{'eval_loss': 0.02202334914666911, 'precision': 0.9438202247191011, 'recall': 0.9563567362428842, 'f1_score': 0.9500471253534402}\n",
      "INFO:ner_model.py:1884:  Converting to features started.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f00b2f24a444442eba454defce2a92a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fef1a39d4b824191add6f436b2fd65e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Norman\\anaconda3\\Lib\\site-packages\\simpletransformers\\ner\\ner_model.py:758: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = amp.GradScaler()\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "019265f12316435ca03fa06e8837fdb1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Epoch 1 of 1:   0%|          | 0/161 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Norman\\anaconda3\\Lib\\site-packages\\simpletransformers\\ner\\ner_model.py:782: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with amp.autocast():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:ner_model.py:1884:  Converting to features started.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31476de520114bd78ec59c45c5010f93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30f0338a464949d4968b78bd7b48ed34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Evaluation:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Norman\\anaconda3\\Lib\\site-packages\\simpletransformers\\ner\\ner_model.py:1303: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with amp.autocast():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:ner_model.py:523:  Training of bert model complete. Saved to outputs/.\n",
      "INFO:ner_model.py:1884:  Converting to features started.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f32681f40b949eeac8b744f8bfaba19",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee0c99f95cb1472c9d5c3338fd53a8d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Evaluation:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:ner_model.py:1253: {'eval_loss': 0.023045973866828717, 'precision': 0.9628975265017667, 'recall': 0.9732142857142857, 'f1_score': 0.9680284191829485}\n",
      "{'eval_loss': 0.023045973866828717, 'precision': 0.9628975265017667, 'recall': 0.9732142857142857, 'f1_score': 0.9680284191829485}\n",
      "Elapsed time: 161.2237 seconds\n"
     ]
    }
   ],
   "source": [
    "def getData(data, group):\n",
    "    group_data = data[data['filename'].isin(group)].copy()\n",
    "    if len(group) == 1:\n",
    "        return group_data\n",
    "    group_data = group_data.sort_values(by=['filename', 'sentence_id'])\n",
    "    current_max_id = 0\n",
    "    for file in group:\n",
    "        group_data.loc[group_data['filename'] == file, 'sentence_id'] += current_max_id\n",
    "        current_max_id = group_data[group_data['filename'] == file]['sentence_id'].max() + 1\n",
    "    group_data.drop('filename', axis=1, inplace=True)\n",
    "    return group_data\n",
    "\n",
    "groups = [['1922-04-24_manual_annotated.json', '1936-09-07_manual_annotated.json'], \n",
    "          ['1927-03-28_manual_annotated.json', '1941-01-03_manual_annotated.json'], \n",
    "          ['1932-01-25_manual_annotated.json'], \n",
    "          ['1934-10-15_manual_annotated.json'], \n",
    "          ['1935-09-30_manual_annotated.json']]\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "transformers_logger = logging.getLogger(\"transformers\")\n",
    "transformers_logger.setLevel(logging.WARNING)\n",
    "\n",
    "labels = ['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC']\n",
    "\n",
    "model_args = NERArgs()\n",
    "model_args.train_batch_size = 16\n",
    "model_args.evaluate_during_training = True\n",
    "model_args.overwrite_output_dir = True\n",
    "model = NERModel(\n",
    "    \"bert\", \"tartuNLP/EstBERT\", args=model_args, labels=labels\n",
    ")\n",
    "data = process_directory_to_dict('data/output4')\n",
    "start_time = time.time()\n",
    "\n",
    "for eval_nr, eval_group in enumerate(groups):\n",
    "    eval_data = getData(data, eval_group)\n",
    "    \n",
    "    all_train_files = []\n",
    "    for i, group in enumerate(groups):\n",
    "        if i != eval_nr:  # Skip the evaluation group\n",
    "            all_train_files.extend(group)\n",
    "    \n",
    "    train_data = getData(data, all_train_files)\n",
    "    model.train_model(train_data, eval_data=eval_data)\n",
    "    result, model_outputs, preds_list = model.eval_model(eval_data)\n",
    "    print(result)\n",
    "    #break\n",
    "\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"Elapsed time: {elapsed_time:.4f} seconds\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
